{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.88888888888889\n",
      "0\n",
      "4.52631578947\n",
      "165.1\n",
      "130\n"
     ]
    }
   ],
   "source": [
    "# This Source Code Form is subject to the terms of the MPL\n",
    "# License. If a copy of the same was not distributed with this\n",
    "# file, You can obtain one at\n",
    "# https://github.com/AkhilHector/pubundsci/blob/master/LICENSE.\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import string\n",
    "from math import sqrt, log\n",
    "from collections import defaultdict\n",
    "from itertools import chain, product\n",
    "from nltk import word_tokenize as tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer as stemmer\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "from nltk.collocations import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "m1 - The number of all word forms a text consists\n",
    "m2 - The sum of the products of each observed frequency to the power of two\n",
    "     and the number of word types observed with that frequency\n",
    "\"\"\"\n",
    "\n",
    "def compute_average_word_length(sentence):\n",
    "    return np.mean([len(words) for words in sentence.split()])\n",
    "\n",
    "def compute_average_sentence_length(sentence):\n",
    "    sentence = re.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", sentence)\n",
    "    return np.mean([len(words) for words in sentence])\n",
    "\n",
    "def freq_of_words_great_sent_len(sentence):\n",
    "    result = []\n",
    "    avg_word_len = compute_average_word_length(sentence)\n",
    "    # sentence = re.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", sentence)\n",
    "    sentence = Counter(sentence.split())\n",
    "    for key, value in sentence.items():\n",
    "        if len(key) > avg_word_len:\n",
    "            result.append(value)\n",
    "#             print (key, value)\n",
    "    return sum(result)\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return re.split(r\"[^0-9A-Za-z\\-'_]+\", sentence)\n",
    "\n",
    "def compute_yules_k_for_text(sentence):\n",
    "    tokens = tokenize(sentence)\n",
    "    counter = Counter(token.upper() for token in tokens)\n",
    "\n",
    "    #compute number of word forms in a given sentence/text\n",
    "    m1 = sum(counter.values())\n",
    "    m2 = sum([frequency ** 2 for frequency in counter.values()])\n",
    "\n",
    "    #compute yules k measure and return the value\n",
    "    yules_k = 10000/((m1 * m1) / (m2 - m1))\n",
    "    return yules_k\n",
    "\n",
    "\n",
    "def words_in_sentence(sentence):\n",
    "    w = [words.strip(\"0123456789!:,.?()[]{}\") for words in sentence.split()]\n",
    "    return filter(lambda x: len(x) > 0, w)\n",
    "\n",
    "def compute_yules_i_for_text(sentence):\n",
    "    dictionary = {}\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    for word in words_in_sentence(sentence):\n",
    "        word = stemmer.stem(word).lower()\n",
    "        try:\n",
    "            dictionary[word] += 1\n",
    "        except:\n",
    "            dictionary[word] = 1\n",
    "\n",
    "    m1 = float(len(dictionary))\n",
    "    m2 = sum([len(list(grouped_values)) * (frequency ** 2) for frequency, grouped_values in groupby(sorted(dictionary.values()))])\n",
    "\n",
    "    # compute yules i and return the value\n",
    "    try:\n",
    "        yules_i = (m1 * m1) / (m2 - m1)\n",
    "        return yules_i\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "def compute_collocation_score(sentence_one, sentence_two, option):\n",
    "    if option == \"bi\":\n",
    "        tokens_for_one = nltk.wordpunct_tokenize(sentence_one)\n",
    "        tokens_for_two = nltk.wordpunct_tokenize(sentence_two)\n",
    "        finder_one = BigramCollocationFinder.from_words(tokens_for_one)\n",
    "        finder_two = BigramCollocationFinder.from_words(tokens_for_two)\n",
    "        result_one = finder_one.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq)\n",
    "        result_one = [(tuple(map(str.lower, values)), scores) for values, scores in result_one]\n",
    "        result_two = finder_two.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq)\n",
    "        result_two = [(tuple(map(str.lower, values)), scores) for values, scores in result_two]\n",
    "        matches = [keys for keys in set(result_one).intersection(set(result_two))]\n",
    "        return len(matches)\n",
    "    elif option == \"tri\":\n",
    "        tokens_for_one = nltk.wordpunct_tokenize(sentence_one)\n",
    "        tokens_for_two = nltk.wordpunct_tokenize(sentence_two)\n",
    "        finder_one = TrigramCollocationFinder.from_words(tokens_for_one)\n",
    "        finder_two = TrigramCollocationFinder.from_words(tokens_for_two)\n",
    "        result_one = finder_one.score_ngrams(nltk.collocations.TrigramAssocMeasures().raw_freq)\n",
    "        result_one = [(tuple(map(str.lower, values)), scores) for values, scores in result_one]\n",
    "        result_two = finder_two.score_ngrams(nltk.collocations.TrigramAssocMeasures().raw_freq)\n",
    "        result_two = [(tuple(map(str.lower, values)), scores) for values, scores in result_two]\n",
    "        matches = [keys for keys in set(result_one).intersection(set(result_two))]\n",
    "        return len(matches)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def vectorize(sentence, vocabulary):\n",
    "    result = [sentence.split().count(i) for i in vocabulary]\n",
    "    return result\n",
    "\n",
    "def convert_words_to_vectors(sentence):\n",
    "    vectorized_sentence = []\n",
    "    vocabulary = sorted(set(chain(*[words.lower().split() for words in sentence])))\n",
    "    for words in sentence:\n",
    "        vectorized_sentence.append((words, vectorize(words, vocabulary)))\n",
    "    return vectorized_sentence, vocabulary\n",
    "\n",
    "def dot_product_of_vectors(vector_one, vector_two):\n",
    "    result = np.dot(vector_one, vector_two) / (sqrt(np.dot(vector_one, vector_one)) * sqrt(np.dot(vector_two, vector_two)))\n",
    "    return result\n",
    "\n",
    "def cosine_sim(sentence_one, sentence_two):\n",
    "    sentences = [sentence_one, sentence_two]\n",
    "    corpus, vocabulary = convert_words_to_vectors(sentences)\n",
    "    similarity = [dot_product_of_vectors(a[1], b[1]) for a, b in product(corpus, corpus)]\n",
    "    return similarity[1]\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "s = \"The smoothing span is given by f. A larger value for f will result in a smoother curve. The number of robustifying iterations is given by iter. The function will run faster with a smaller number of iterations.\"\n",
    "a = \"Although population-level genomic sequence data have been gathered extensively for humans similar data from our closest living relatives are just beginning to emerge. Examination of genomic variation within great apes offers many opportunities to increase our understanding of the forces that have differentially shaped the evolutionary history of hominid taxa. Here we expand upon the work of the Great Ape Genome Project by analyzing medium to high coverage whole-genome sequences from 14 western lowland gorillas (Gorilla gorilla gorilla) 2 eastern lowland gorillas (G. beringei graueri) and a single Cross River individual (G. gorilla diehli). We infer that the ancestors of western and eastern lowland gorillas diverged from a common ancestor approximately 261 ka and that the ancestors of the Cross River population diverged from the western lowland gorilla lineage approximately 68 ka. Using a diffusion approximation approach to model the genome-wide site frequency spectrum we infer a history of western lowland gorillas that includes an ancestral population expansion of 1.4-fold around 970 ka and a recent 5.6-fold contraction in population size 23 ka. The latter may correspond to a major reduction in African equatorial forests around the Last Glacial Maximum. We also analyze patterns of variation among western lowland gorillas to identify several genomic regions with strong signatures of recent selective sweeps. We find that processes related to taste pancreatic and saliva secretion sodium ion transmembrane transport and cardiac muscle function are overrepresented in genomic regions predicted to have experienced recent positive selection.\"\n",
    "b = \"Please note: this list is intended as a resource for those of you who are interested in responding to other CEHG members&rsquo; works for the blog. Let the blog editor (kkanagaw@stanford.edu) know if there are other publications that should be added to the\"\n",
    "print (compute_yules_i_for_text(s))\n",
    "print (compute_collocation_score(a, b, \"bi\"))\n",
    "print (compute_average_word_length(s))\n",
    "print (compute_average_sentence_length(a))\n",
    "print (freq_of_words_great_sent_len(a))\n",
    "# else:\n",
    "#     sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"combined_dataset_alpha.csv\")\n",
    "raw_data = raw_data.sample(frac=0.01).reset_index(drop=True)\n",
    "raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'n',\n",
       " 'e',\n",
       " ' ',\n",
       " ' ',\n",
       " 'k',\n",
       " 'h',\n",
       " 'l',\n",
       " ' ',\n",
       " 'n',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'r',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " ' ',\n",
       " 'c',\n",
       " 'p',\n",
       " 'u',\n",
       " 'e',\n",
       " ' ',\n",
       " 'e',\n",
       " 'h',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'w',\n",
       " 'n',\n",
       " 'e',\n",
       " 'r',\n",
       " 'f',\n",
       " 'u',\n",
       " 'l']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"my name is akhil and i am trying to compute something wonderful\"\n",
    "remove_stopwords(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yules I Measure on Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "raw_data[\"yules_i_measure_abs\"] = raw_data[\"altmetric_id\"]\n",
    "for each in range(0, len(raw_data[\"altmetric_id\"])):\n",
    "        raw_data[\"yules_i_measure_abs\"][each] = compute_yules_i_for_text(raw_data[\"abstract\"][each])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yules I Measure on Blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "raw_data[\"yules_i_measure_blg\"] = raw_data[\"altmetric_id\"]\n",
    "for each in range(0, len(raw_data[\"altmetric_id\"])):\n",
    "        raw_data[\"yules_i_measure_blg\"][each] = compute_yules_i_for_text(raw_data[\"blog_post\"][each])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Average word length on Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "raw_data[\"avg_word_len_abs\"] = raw_data[\"altmetric_id\"]\n",
    "for each in range(0, len(raw_data[\"altmetric_id\"])):\n",
    "        raw_data[\"avg_word_len_abs\"][each] = compute_average_word_length(raw_data[\"abstract\"][each])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average sentence length on Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "raw_data[\"avg_sen_len_abs\"] = raw_data[\"altmetric_id\"]\n",
    "for each in range(0, len(raw_data[\"altmetric_id\"])):\n",
    "        raw_data[\"avg_sen_len_abs\"][each] = compute_average_sentence_length(raw_data[\"abstract\"][each])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency of words greater than avg word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "raw_data[\"freq_words_gt_sen_len_abs\"] = raw_data[\"altmetric_id\"]\n",
    "for each in range(0, len(raw_data[\"altmetric_id\"])):\n",
    "        raw_data[\"freq_words_gt_sen_len_abs\"][each] = freq_of_words_great_sent_len(raw_data[\"abstract\"][each])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocation Similarity between Abstract and Blog Post Summary Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "raw_data[\"collocation_sim_bi\"] = raw_data[\"altmetric_id\"]\n",
    "for each in range(0, len(raw_data[\"altmetric_id\"])):\n",
    "        raw_data[\"collocation_sim_bi\"][each] = compute_collocation_score(raw_data[\"abstract\"][each], raw_data[\"blog_post\"][each], \"bi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocation Similarity between Abstract and Blog Post Summary Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "raw_data[\"collocation_sim_tri\"] = raw_data[\"altmetric_id\"]\n",
    "for each in range(0, len(raw_data[\"altmetric_id\"])):\n",
    "        raw_data[\"collocation_sim_tri\"][each] = compute_collocation_score(raw_data[\"abstract\"][each], raw_data[\"blog_post\"][each], \"tri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:126: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "raw_data[\"cosine_sem_sim\"] = raw_data[\"altmetric_id\"]\n",
    "for each in range(0, len(raw_data[\"altmetric_id\"])):\n",
    "        raw_data[\"cosine_sem_sim\"][each] = cosine_sim(raw_data[\"abstract\"][each], raw_data[\"blog_post\"][each])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data.to_csv(\"regression_sample_beta.csv\", sep = ',', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
